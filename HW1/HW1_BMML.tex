\documentclass{article}

\title{EECS E6892 Bayesian Models fo Machine Learning\\ Homework 1}
\author{John Min; jcm2199}
\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx, array, pdfpages}

\begin{document}
\maketitle
\section{Monty Hall Problem}

After the host opens one of the doors, she should switch.  Assuming that she switches, she has a two-thirds chance of winning the prize versus just a one-third probability of taking the prize by staying with her original selection.  We illustrate the posterior probabilities below.  Without loss of generality, we can assume that she chooses Door 1, initially.

\section{Dirichlet-Multinomial}

Let $\pi = (\pi_1, ..., \pi_K)$ with $\pi_j \geq 0$, $\sum_j \pi_j = 1$.  $X_i ~ \text{Multinomial} (\pi)$ iid.

\section{Normal-Inverse-Gamma}
$x_i ~ \text{Normal}(\mu, \lambda^{-1})$ where $\mu | \lambda ~ \text{Normal}(0, a \lambda^{-1}$ and $\lambda ~ \text{Gamma}(b,c)$.  


\section{MAP for multinomial logistic - MNIST data}
\end{document}

\documentclass{article}

\author{John Min}
\title{EECS E6892 Bayesian Models for Machine Learning \\ Homework 4}

\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx, array, pdfpages}

\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\Rn}{\R^n}
\newcommand{\Rm}{\R^m}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\grad}{\nabla}
\newcommand{\Rnn}{\R^{n\times n}}
\newcommand{\map}[3]{#1:#2\rightarrow #3}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\tpose}[1]{#1^{\scriptscriptstyle T}}
\newcommand{\indicator}[2]{\delta\left(#1 \mid #2 \right)}

\DeclareMathOperator{\Tr}{Tr} 

\begin{document}
\maketitle

\newpage
\section{Normal-Wishart prior}

\noindent
The multivariate analog ofthe normal-gamma prior is the normal-Wishart prior. \\

\noindent
We are given observations $x_1, \ldots, x_n$ from a $d$-dimensional multivariate Gaussian with a Normal-Wishart prior on the mean and precision matrix:  $x_i \sim N(\mu, \Lambda^{-1}), \mu | \Lambda \sim N(m, (\frac{1}{a} \Lambda)^{-1}), \Lambda \sim Wishart(\nu, B).$  \\

\subsection{Likelihood $p(D | \mu, \Lambda)$}
\begin{align*}
p(D | \mu, \Lambda) &= p(x_1, \ldots, x_N | \mu, \Lambda) \\
					&= (2\pi)^{-nd/2} |\Lambda|^{n/2} \exp\Big\{ -\frac{1}{2} \displaystyle \sum_{i=1}^N (x_i - \mu)^\top \Lambda (x_i - \mu) \Big\}
\end{align*}

\subsection{Prior $p(\mu, \Lambda)$}
\begin{align*}
p(\mu, \Lambda) &= p(\mu | \Lambda) p(\Lambda) \\
				&= N(\mu | m, (\frac{1}{a}\Lambda)^{-1}) \cdot \text{Wishart}(B,\nu) \\
				&= (2a \pi)^{-d/2} \vert \Lambda \vert^{1/2} \exp \Big\{-\frac{1}{2a} (\mu-m)^\top \Lambda (\mu-m) \Big\} \Lambda^{(\nu-d-1)/2} \exp \Big\{-\frac{1}{2} \tr(B^{-1}\Lambda) \Big \}
\end{align*}

\subsection{Posterior $p(\mu, \Lambda | x_1, \ldots, x_N)$}
\begin{align*}
p(\mu, \Lambda | x_1, \ldots, x_N) &= \displaystyle \prod_{i=1}^N N(x_i | \mu, \Lambda^{-1}) N(\mu | m, (\frac{1}{a} \Lambda)^{-1}) \cdot \text{Wishart}(B, \nu) \\
&= \text{const} \cdot \exp \bigg\{-\frac{1}{2} \Big[\displaystyle \sum_{i=1}^N \big(x_i \Lambda x_i\big) - 2n\bar{x}^\top \Lambda \mu + n \mu^\top \Lambda \mu + \frac{1}{a} \mu \Lambda \mu - \frac{2}{a} \mu^\top \Lambda \mu + \frac{1}{a} m^\top \Lambda m \Big] \bigg\} \cdot \exp\Big\{-\frac{1}{2} \tr(B^{-1}\Lambda)\Big\}
&= \text{const} \cdot \exp \bigg\{-\frac{1}{2} \Big[\mu^\top (n+a^{-1})\Lambda\mu - 2\mu^\top (n+a^{-1}) \Lambda \Big[\frac{n\bar{x} + a^{-1}m}{n+a^{-1}} + \text{const} \bigg\} \\
&= N(\mu_n, \Lambda_n) \cdot \text{Wishart}(B_n, \nu_n)
\end{align*}
where \mu_n = \frac{a^{-1}m + n\bar x}{a^{-1}+n}
\Lambda_n = \Big[(a^{-1} + n) \Lambda \Big]^{-1}

\subsection{Marginal likelihood of a singular data point}
For a single vector, $p(x) = \int_\Lambda \int_\mu p(x|\mu, \Lambda) p(\mu|\Lambda) p(\Lambda) \partial \mu \partial \Lambda.$ \\

\noindent
We see that we can complete the square and integrate out a Normal-Wishart distribution leaving us with a normalization ratio.\\

$$p(x) = \pi^{-d/2} \frac{\Gamma}{} \Bigg( \frac{}{} \Bigg )^{d/2}$$



\newpage
\section{Expectation Maximization for Gaussian Mixture Models}

\subsection{The EM algorithm}
\begin{enumerate}
	\item initialize $\mu_k$, covariances $\Sigma_k$, mixing coefficients $\pi_k$, and evaluate the initia value of the log-likelihood
	\item \textbf{(E-step)}: evaluate responsibilities\\
$\gamma(z_{nk}) \leftarrow \pi_k \cdot N(x_n | \mu_k, \Sigma_k) / \displaystyle \sum_{j=1}^K \pi_j N(x_n | \mu_j, \Sigma_j)$ \\
		\item \textbf{(M-step)}: re-estimate parameters using current responsibilities\\
	$\mu_k^{\text{new}} \leftarrow \frac{1}{N_k} \displaystyle \sum_{n=1}^N \gamma(z_{nk}) x_n$ \\
$\Sigma_k^{\text{new}} \leftarrow \frac{1}{N_k} \displaystyle \sum_{n=1}^N \gamma(z_{nk}) \Big(x_n - \mu_k^\text{new}\Big)\Big(x_n - \mu_k^\text{new}\Big)^\top$ \\
$ \pi_k^\text{new} \leftarrow \frac{N_k}{N}$ where $N_k = \displaystyle \sum_{n=1}^N \gamma(z_{nk})$

\item Evaluate log-likelihood \\
$\ln p(x | \mu, \Sigma, \pi) = \displaystyle \sum_{n=1}^N  \ln \Big\{ \sum_{k=1}^K \pi_k N(x_n | \mu_k, \Sigma_k \Big\} $

\end{enumerate}

\subsection{Plots of log-likelihood}
\subsubsection{K=2}
\subsubsection{K=4}
\subsubsection{K=6}
\subsubsection{K=8}
\subsubsection{K=10}

\newpage
\section{Dirichlet Process Gaussian Mixture Model (D.P.G.M.M.)}
\subsection{Normal-Wishart prior}
\end{document}

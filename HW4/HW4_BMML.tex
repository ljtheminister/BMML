\documentclass{article}

\author{John Min}
\title{EECS E6892 Bayesian Models for Machine Learning \\ Homework 4}

\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx, array, pdfpages}

\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\Rn}{\R^n}
\newcommand{\Rm}{\R^m}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\grad}{\nabla}
\newcommand{\Rnn}{\R^{n\times n}}
\newcommand{\map}[3]{#1:#2\rightarrow #3}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\tpose}[1]{#1^{\scriptscriptstyle T}}
\newcommand{\indicator}[2]{\delta\left(#1 \mid #2 \right)}

\DeclareMathOperator{\Tr}{Tr} 

\begin{document}
\maketitle


\section{Normal-Wishart prior}

\noindent
The multivariate analog ofthe normal-gamma prior is the normal-Wishart prior. \\

\noindent
We are given observations $x_1, \ldots, x_n$ from a $d$-dimensional multivariate Gaussian with a Normal-Wishart prior on the mean and precision matrix:  $x_i \sim N(\mu, \Lambda^{-1}), \mu | \Lambda \sim N(m, (\frac{1}{a} \Lambda)^{-1}), \Lambda \sim Wishart(\nu, B).$  \\

\subsection{Posterior $p(\mu, \Lambda | x_1, \ldots, x_N)$}

\subsection{Marginal likelihood of a singular data point}
For a single vector, $p(x) = \int_\Lambda \int_\mu p(x|\mu, \Lambda) p(\mu|\Lambda) p(\Lambda) \partial \mu \partial \Lambda.$ \\

$$p(x) = \pi^{-d/2} \frac{\Gamma}{} \Bigg( \frac{}{} \Bigg )^{d/2}$$




\section{EM for Gaussian mixture}

\section{Dirichlet Process Gaussian Mixture Model (D.P.G.M.M.)}
\subsection{Normal-Wishart prior}
\end{document}

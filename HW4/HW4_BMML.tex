\documentclass{article}

\author{John Min}
\title{EECS E6892 Bayesian Models for Machine Learning \\ Homework 4}

\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb, amsmath, parallel, mathtools, graphicx, array, pdfpages}

\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\Rn}{\R^n}
\newcommand{\Rm}{\R^m}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\grad}{\nabla}
\newcommand{\Rnn}{\R^{n\times n}}
\newcommand{\map}[3]{#1:#2\rightarrow #3}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\tpose}[1]{#1^{\scriptscriptstyle T}}
\newcommand{\indicator}[2]{\delta\left(#1 \mid #2 \right)}

\DeclareMathOperator{\Tr}{Tr} 

\begin{document}
\maketitle


\section{Normal-Wishart prior}

\noindent
The multivariate analog ofthe normal-gamma prior is the normal-Wishart prior. \\

\noindent
We are given observations $x_1, \ldots, x_n$ from a $d$-dimensional multivariate Gaussian with a Normal-Wishart prior on the mean and precision matrix:  $x_i \sim N(\mu, \Lambda^{-1}), \mu | \Lambda \sim N(m, (\frac{1}{a} \Lambda)^{-1}), \Lambda \sim Wishart(\nu, B).$  \\

\subsection{Posterior $p(\mu, \Lambda | x_1, \ldots, x_n)$}

\begin{align*}
p(\mu, \Lambda | x_1, \ldots, x_N) &= \\
& \sim N \Big(\mu \; \Bigr | \; \frac{an\bar x + m}{an+1}, \big[(n+\frac{1}{a})\Lambda \big]^{-1} \Big) \cdot W(B_n, \nu_n) \\
& \sim NW(\mu_n, \Lambda_n, B_n, \nu_n)
\end{align*}
where $\mu_n = \frac{an \bar x + m}{an+1}$, $\Lambda_n = \big[(n+\frac{1}{a})\Gamma\big]^{-1}$, $B_n = \Big[ B^{-1} + \displaystyle \sum_i^N (x_i - \bar x) (x_i - \bar x)^\top + \frac{n}{an+1} (\bar x - m)(\bar x - m)^\top \Big]^{-1}$, $\nu_n = \nu + n$.





\subsection{Marginal likelihood of a singular data point}
For a single vector, $p(x) = \int_\Lambda \int_\mu p(x|\mu, \Lambda) p(\mu|\Lambda) p(\Lambda) \partial \mu \partial \Lambda.$ \\

\noindent
Given this integral, we can integrate out a Normal-Wishart distribution and thus, we are left with a normalization ratio. \\

$$p(x) = \pi^{-d/2} \frac{\Gamma}{} \Bigg( \frac{}{} \Bigg )^{d/2}$$




\section{Expectation Maximization for Gaussian Mixture Models}

\subsection{The EM algorithm}
\begin{enumerate}
	\item initialize $\mu_k$, covariances $\Sigma_k$, mixing coefficients $\pi_k$, and evaluate the initia value of the log-likelihood
	\item \textbf{(E-step)}: evaluate responsibilities\\
$\gamma(z_{nk}) \leftarrow \pi_k \cdot N(x_n | \mu_k, \Sigma_k) / \displaystyle \sum_{j=1}^K \pi_j N(x_n | \mu_j, \Sigma_j)$ \\
		\item \textbf{(M-step)}: re-estimate parameters using current responsibilities\\
	$\mu_k^{\text{new}} \leftarrow \frac{1}{N_k} \displaystyle \sum_{n=1}^N \gamma(z_{nk}) x_n$ \\
$\Sigma_k^{\text{new}} \leftarrow \frac{1}{N_k} \displaystyle \sum_{n=1}^N \gamma(z_{nk}) \Big(x_n - \mu_k^\text{new}\Big)\Big(x_n - \mu_k^\text{new}\Big)^\top$ \\
$ \pi_k^\text{new} \leftarrow \frac{N_k}{N}$ where $N_k = \displaystyle \sum_{n=1}^N \gamma(z_{nk})$

\item Evaluate log-likelihood \\
$\ln p(x | \mu, \Sigma, \pi) = \displaystyle \sum_{n=1}^N  \ln \Big\{ \sum_{k=1}^K \pi_k N(x_n | \mu_k, \Sigma_k \Big\} $

\end{enumerate}

\subsection{Plots of log-likelihood}
\subsubsection{K=2}
\subsubsection{K=4}
\subsubsection{K=6}
\subsubsection{K=8}
\subsubsection{K=10}


\section{Dirichlet Process Gaussian Mixture Model (D.P.G.M.M.)}
\subsection{Normal-Wishart prior}
\end{document}
